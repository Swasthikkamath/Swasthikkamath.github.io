<!DOCTYPE html>
<html lang="en">

  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Recurrent Neural Networks in Tensorflow II | Your awesome title</title>
<meta name="generator" content="Jekyll v4.2.1">
<meta property="og:title" content="Recurrent Neural Networks in Tensorflow II">
<meta property="og:locale" content="en_US">
<meta name="description" content="This is the second in a series of posts about recurrent neural networks in Tensorflow. The first post lives here. In this post, we will build upon our vanilla RNN by learning how to use Tensorflow’s scan and dynamic_rnn models, upgrading the RNN cell and stacking multiple RNNs, and adding dropout and layer normalization. We will then use our upgraded RNN to generate some text, character by character.">
<meta property="og:description" content="This is the second in a series of posts about recurrent neural networks in Tensorflow. The first post lives here. In this post, we will build upon our vanilla RNN by learning how to use Tensorflow’s scan and dynamic_rnn models, upgrading the RNN cell and stacking multiple RNNs, and adding dropout and layer normalization. We will then use our upgraded RNN to generate some text, character by character.">
<link rel="canonical" href="https://aadikuchlous.github.io/jekyll/tensorflow/neuralnetworks/2022/01/04/recurrent-neural-networks-in-tensorflow-ii.html">
<meta property="og:url" content="https://aadikuchlous.github.io/jekyll/tensorflow/neuralnetworks/2022/01/04/recurrent-neural-networks-in-tensorflow-ii.html">
<meta property="og:site_name" content="Your awesome title">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2022-01-04T11:28:34+05:30">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="Recurrent Neural Networks in Tensorflow II">
<script type="application/ld+json">
{"datePublished":"2022-01-04T11:28:34+05:30","dateModified":"2022-01-04T11:28:34+05:30","headline":"Recurrent Neural Networks in Tensorflow II","url":"https://aadikuchlous.github.io/jekyll/tensorflow/neuralnetworks/2022/01/04/recurrent-neural-networks-in-tensorflow-ii.html","mainEntityOfPage":{"@type":"WebPage","@id":"https://aadikuchlous.github.io/jekyll/tensorflow/neuralnetworks/2022/01/04/recurrent-neural-networks-in-tensorflow-ii.html"},"description":"This is the second in a series of posts about recurrent neural networks in Tensorflow. The first post lives here. In this post, we will build upon our vanilla RNN by learning how to use Tensorflow’s scan and dynamic_rnn models, upgrading the RNN cell and stacking multiple RNNs, and adding dropout and layer normalization. We will then use our upgraded RNN to generate some text, character by character.","@type":"BlogPosting","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css">
<link type="application/atom+xml" rel="alternate" href="https://aadikuchlous.github.io/feed.xml" title="Your awesome title">
<script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"svg":{"fontCache":"global"}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<header class="site-header" role="banner">

  <div class="wrapper">
<a class="site-title" rel="author" href="/">Your awesome title</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger">
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewbox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"></path>
            </svg>
          </span>
        </label>

        <div class="trigger">
<a class="page-link" href="/about/">About</a><a class="page-link" href="/test/">TestPage</a>
</div>
      </nav>
</div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Recurrent Neural Networks in Tensorflow II</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2022-01-04T11:28:34+05:30" itemprop="datePublished">Jan 4, 2022
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>This is the second in a series of posts about recurrent neural networks in Tensorflow. The first post lives <a href="https://r2rt.com/recurrent-neural-networks-in-tensorflow-i.html">here</a>. In this post, we will build upon our vanilla RNN by learning how to use Tensorflow’s scan and dynamic_rnn models, upgrading the RNN cell and stacking multiple RNNs, and adding dropout and layer normalization. We will then use our upgraded RNN to generate some text, character by character.</p>

<p><strong>Note 3/14/2017:</strong> This tutorial is quite a bit deprecated by changes to the TF api. Leaving it up since it may still be useful, and most changes to the API are cosmetic (biggest change is that many of the RNN cells and functions are in the tf.contrib.rnn module). There was also a change to the ptb_iterator. A (slightly modified) copy of the old version which should work until I update this tutorial is uploaded <a href="https://gist.github.com/spitis/2dd1720850154b25d2cec58d4b75c4a0">here</a>.</p>

<h3 id="recap-of-our-model">Recap of our model</h3>

<p>In the last post, we built a very simple, no frills RNN that was quickly able to learn to solve the toy task we created for it.</p>

<p>Here is the formal statement of our model from last time:</p>

<p>$ S_t = \text{tanh}(W(X_t \ @ \ S_{t-1}) + b_s) $</p>

<p>$ P_t = \text{softmax}(US_t + b_p) $</p>

<p>where $ @ $ represents vector concatenation, $ X_t \in R^n $ is an input vector, $ W \in R^{d \times (n + d)}, \  b_s \in R^d, \ U \in R^{n \times d} $ is the size of the input and output vectors, and d is the size of the hidden state vector. At time step 0, $ S_{-1} $ (the initial state) is initialized as a vector of zeros.</p>

<h3 id="task-and-data">Task and data</h3>

<p>This time around we will be building a character-level language model to generate character sequences, a la Andrej Karpathy’s <a href="https://github.com/karpathy/char-rnn">char-rnn</a> (and see, e.g., a Tensorflow implementation by Sherjil Ozair <a href="https://github.com/sherjilozair/char-rnn-tensorflow">here</a>).</p>

<p>Why do something that’s already been done? Well, this is a much harder task than the toy model from last time. This model needs to handle long sequences and learn long time dependencies. That makes a great task for learning about adding features to our RNN, and seeing how our changes affect the results as we go.</p>

<p>To start, let’s create our data generator. We’ll use the tiny-shakespeare corpus as our data, though we could use any plain text file. We’ll choose to use all of the characters in the text file as our vocabulary, treating lowercase and capital letters are separate characters. In practice, there may be some advantage to forcing the network to use similar representations for capital and lowercase letters by using the same one-hot representations for each, plus a binary flag to indicate whether or not the letter is a capital. Additionally, it is likely a good idea to restrict the vocabulary (i.e., the set of characters) used, by replacing uncommon characters with an UNK token (like a square: □).</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="s">"""
Imports
"""</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">urllib.request</span>
<span class="kn">from</span> <span class="nn">tensorflow.models.rnn.ptb</span> <span class="kn">import</span> <span class="n">reader</span></code></pre></figure>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="s">"""
Load and process data, utility functions
"""</span>

<span class="n">file_url</span> <span class="o">=</span> <span class="s">'https://raw.githubusercontent.com/jcjohnson/torch-rnn/master/data/tiny-shakespeare.txt'</span>
<span class="n">file_name</span> <span class="o">=</span> <span class="s">'tinyshakespeare.txt'</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">exists</span><span class="p">(</span><span class="n">file_name</span><span class="p">):</span>
    <span class="n">urllib</span><span class="p">.</span><span class="n">request</span><span class="p">.</span><span class="n">urlretrieve</span><span class="p">(</span><span class="n">file_url</span><span class="p">,</span> <span class="n">file_name</span><span class="p">)</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">file_name</span><span class="p">,</span><span class="s">'r'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">raw_data</span> <span class="o">=</span> <span class="n">f</span><span class="p">.</span><span class="n">read</span><span class="p">()</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Data length:"</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">raw_data</span><span class="p">))</span>

<span class="n">vocab</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">raw_data</span><span class="p">)</span>
<span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>
<span class="n">idx_to_vocab</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">enumerate</span><span class="p">(</span><span class="n">vocab</span><span class="p">))</span>
<span class="n">vocab_to_idx</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">idx_to_vocab</span><span class="p">.</span><span class="n">values</span><span class="p">(),</span> <span class="n">idx_to_vocab</span><span class="p">.</span><span class="n">keys</span><span class="p">()))</span>

<span class="n">data</span> <span class="o">=</span> <span class="p">[</span><span class="n">vocab_to_idx</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">raw_data</span><span class="p">]</span>
<span class="k">del</span> <span class="n">raw_data</span>

<span class="k">def</span> <span class="nf">gen_epochs</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="k">yield</span> <span class="n">reader</span><span class="p">.</span><span class="n">ptb_iterator</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">reset_graph</span><span class="p">():</span>
    <span class="k">if</span> <span class="s">'sess'</span> <span class="ow">in</span> <span class="nb">globals</span><span class="p">()</span> <span class="ow">and</span> <span class="n">sess</span><span class="p">:</span>
        <span class="n">sess</span><span class="p">.</span><span class="n">close</span><span class="p">()</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">reset_default_graph</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">train_network</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">,</span> <span class="n">num_steps</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span> <span class="n">verbose</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">save</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">set_random_seed</span><span class="p">(</span><span class="mi">2345</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
        <span class="n">sess</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">initialize_all_variables</span><span class="p">())</span>
        <span class="n">training_losses</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">gen_epochs</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)):</span>
            <span class="n">training_loss</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">steps</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">training_state</span> <span class="o">=</span> <span class="bp">None</span>
            <span class="k">for</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="ow">in</span> <span class="n">epoch</span><span class="p">:</span>
                <span class="n">steps</span> <span class="o">+=</span> <span class="mi">1</span>

                <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">g</span><span class="p">[</span><span class="s">'x'</span><span class="p">]:</span> <span class="n">X</span><span class="p">,</span> <span class="n">g</span><span class="p">[</span><span class="s">'y'</span><span class="p">]:</span> <span class="n">Y</span><span class="p">}</span>
                <span class="k">if</span> <span class="n">training_state</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
                    <span class="n">feed_dict</span><span class="p">[</span><span class="n">g</span><span class="p">[</span><span class="s">'init_state'</span><span class="p">]]</span> <span class="o">=</span> <span class="n">training_state</span>
                <span class="n">training_loss_</span><span class="p">,</span> <span class="n">training_state</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">sess</span><span class="p">.</span><span class="n">run</span><span class="p">([</span><span class="n">g</span><span class="p">[</span><span class="s">'total_loss'</span><span class="p">],</span>
                                                      <span class="n">g</span><span class="p">[</span><span class="s">'final_state'</span><span class="p">],</span>
                                                      <span class="n">g</span><span class="p">[</span><span class="s">'train_step'</span><span class="p">]],</span>
                                                             <span class="n">feed_dict</span><span class="p">)</span>
                <span class="n">training_loss</span> <span class="o">+=</span> <span class="n">training_loss_</span>
            <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                <span class="k">print</span><span class="p">(</span><span class="s">"Average training loss for Epoch"</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="s">":"</span><span class="p">,</span> <span class="n">training_loss</span><span class="o">/</span><span class="n">steps</span><span class="p">)</span>
            <span class="n">training_losses</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">training_loss</span><span class="o">/</span><span class="n">steps</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">save</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">g</span><span class="p">[</span><span class="s">'saver'</span><span class="p">].</span><span class="n">save</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="n">save</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">training_losses</span></code></pre></figure>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">Data</span> <span class="n">length</span><span class="p">:</span> <span class="mi">1115394</span></code></pre></figure>

<h3 id="using-tfscan-and-dynamic_rnn-to-speed-things-up">Using tf.scan and dynamic_rnn to speed things up</h3>

<p>Recall from <a href="https://r2rt.com/recurrent-neural-networks-in-tensorflow-i.html">last post</a> that we represented each duplicate tensor of our RNN (e.g., the rnn inputs, rnn outputs, the predictions and the loss) as a list of tensors:</p>

<p><img src="https://r2rt.com/static/images/BasicRNNLabeled.png" alt=""></p>

<p>This worked quite well for our toy task, because our longest dependency was 7 steps back and we never really needed to backpropagate errors more than 10 steps. Even with a word-level RNN, using lists will probably be sufficient. See, e.g., my post on <a href="http://r2rt.com/styles-of-truncated-backpropagation.html">Styles of Truncated Backpropagation</a>, where I build a 40-step graph with no problems. But for a character-level model, 40 characters isn’t a whole lot. We might want to capture much longer dependencies. So let’s see what happens when we build a graph that is 200 time steps wide:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">build_basic_rnn_graph_with_list</span><span class="p">(</span>
    <span class="n">state_size</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
    <span class="n">num_classes</span> <span class="o">=</span> <span class="n">vocab_size</span><span class="p">,</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
    <span class="n">num_steps</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span>
    <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-4</span><span class="p">):</span>

    <span class="n">reset_graph</span><span class="p">()</span>

    <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">int32</span><span class="p">,</span> <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s">'input_placeholder'</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">int32</span><span class="p">,</span> <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s">'labels_placeholder'</span><span class="p">)</span>

    <span class="n">x_one_hot</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
    <span class="n">rnn_inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="n">squeeze_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">tf</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">,</span> <span class="n">x_one_hot</span><span class="p">)]</span>

    <span class="n">cell</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">rnn_cell</span><span class="p">.</span><span class="n">BasicRNNCell</span><span class="p">(</span><span class="n">state_size</span><span class="p">)</span>
    <span class="n">init_state</span> <span class="o">=</span> <span class="n">cell</span><span class="p">.</span><span class="n">zero_state</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">rnn_outputs</span><span class="p">,</span> <span class="n">final_state</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">rnn</span><span class="p">(</span><span class="n">cell</span><span class="p">,</span> <span class="n">rnn_inputs</span><span class="p">,</span> <span class="n">initial_state</span><span class="o">=</span><span class="n">init_state</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s">'softmax'</span><span class="p">):</span>
        <span class="n">W</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">'W'</span><span class="p">,</span> <span class="p">[</span><span class="n">state_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">])</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">'b'</span><span class="p">,</span> <span class="p">[</span><span class="n">num_classes</span><span class="p">],</span> <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">constant_initializer</span><span class="p">(</span><span class="mf">0.0</span><span class="p">))</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">rnn_output</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span> <span class="k">for</span> <span class="n">rnn_output</span> <span class="ow">in</span> <span class="n">rnn_outputs</span><span class="p">]</span>

    <span class="n">y_as_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">squeeze_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">tf</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">,</span> <span class="n">y</span><span class="p">)]</span>

    <span class="n">loss_weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="p">.</span><span class="n">ones</span><span class="p">([</span><span class="n">batch_size</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_steps</span><span class="p">)]</span>
    <span class="n">losses</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">seq2seq</span><span class="p">.</span><span class="n">sequence_loss_by_example</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y_as_list</span><span class="p">,</span> <span class="n">loss_weights</span><span class="p">)</span>
    <span class="n">total_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>
    <span class="n">train_step</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">).</span><span class="n">minimize</span><span class="p">(</span><span class="n">total_loss</span><span class="p">)</span>

    <span class="k">return</span> <span class="nb">dict</span><span class="p">(</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">,</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">,</span>
        <span class="n">init_state</span> <span class="o">=</span> <span class="n">init_state</span><span class="p">,</span>
        <span class="n">final_state</span> <span class="o">=</span> <span class="n">final_state</span><span class="p">,</span>
        <span class="n">total_loss</span> <span class="o">=</span> <span class="n">total_loss</span><span class="p">,</span>
        <span class="n">train_step</span> <span class="o">=</span> <span class="n">train_step</span>
    <span class="p">)</span></code></pre></figure>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">t</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">build_basic_rnn_graph_with_list</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="s">"It took"</span><span class="p">,</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t</span><span class="p">,</span> <span class="s">"seconds to build the graph."</span><span class="p">)</span></code></pre></figure>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">It</span> <span class="n">took</span> <span class="mf">5.626644849777222</span> <span class="n">seconds</span> <span class="n">to</span> <span class="n">build</span> <span class="n">the</span> <span class="n">graph</span><span class="p">.</span></code></pre></figure>

<p>It took over 5 seconds to build the graph of the most basic RNN model! This could bad… what happens when we move up to a 3-layer LSTM?</p>

<p>Below, we switch out the RNN cell for a Multi-layer LSTM cell. We’ll go over the details of how to do this in the next section.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">build_multilayer_lstm_graph_with_list</span><span class="p">(</span>
    <span class="n">state_size</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
    <span class="n">num_classes</span> <span class="o">=</span> <span class="n">vocab_size</span><span class="p">,</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
    <span class="n">num_steps</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span>
    <span class="n">num_layers</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
    <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-4</span><span class="p">):</span>

    <span class="n">reset_graph</span><span class="p">()</span>

    <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">int32</span><span class="p">,</span> <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s">'input_placeholder'</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">int32</span><span class="p">,</span> <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s">'labels_placeholder'</span><span class="p">)</span>

    <span class="n">embeddings</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">'embedding_matrix'</span><span class="p">,</span> <span class="p">[</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">state_size</span><span class="p">])</span>
    <span class="n">rnn_inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">tf</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span>
                                <span class="n">num_steps</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">embedding_lookup</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">x</span><span class="p">))]</span>

    <span class="n">cell</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">rnn_cell</span><span class="p">.</span><span class="n">LSTMCell</span><span class="p">(</span><span class="n">state_size</span><span class="p">,</span> <span class="n">state_is_tuple</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">cell</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">rnn_cell</span><span class="p">.</span><span class="n">MultiRNNCell</span><span class="p">([</span><span class="n">cell</span><span class="p">]</span> <span class="o">*</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">state_is_tuple</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">init_state</span> <span class="o">=</span> <span class="n">cell</span><span class="p">.</span><span class="n">zero_state</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">rnn_outputs</span><span class="p">,</span> <span class="n">final_state</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">rnn</span><span class="p">(</span><span class="n">cell</span><span class="p">,</span> <span class="n">rnn_inputs</span><span class="p">,</span> <span class="n">initial_state</span><span class="o">=</span><span class="n">init_state</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s">'softmax'</span><span class="p">):</span>
        <span class="n">W</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">'W'</span><span class="p">,</span> <span class="p">[</span><span class="n">state_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">])</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">'b'</span><span class="p">,</span> <span class="p">[</span><span class="n">num_classes</span><span class="p">],</span> <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">constant_initializer</span><span class="p">(</span><span class="mf">0.0</span><span class="p">))</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">rnn_output</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span> <span class="k">for</span> <span class="n">rnn_output</span> <span class="ow">in</span> <span class="n">rnn_outputs</span><span class="p">]</span>

    <span class="n">y_as_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">squeeze_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">tf</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">,</span> <span class="n">y</span><span class="p">)]</span>

    <span class="n">loss_weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="p">.</span><span class="n">ones</span><span class="p">([</span><span class="n">batch_size</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_steps</span><span class="p">)]</span>
    <span class="n">losses</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">seq2seq</span><span class="p">.</span><span class="n">sequence_loss_by_example</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y_as_list</span><span class="p">,</span> <span class="n">loss_weights</span><span class="p">)</span>
    <span class="n">total_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>
    <span class="n">train_step</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">).</span><span class="n">minimize</span><span class="p">(</span><span class="n">total_loss</span><span class="p">)</span>

    <span class="k">return</span> <span class="nb">dict</span><span class="p">(</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">,</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">,</span>
        <span class="n">init_state</span> <span class="o">=</span> <span class="n">init_state</span><span class="p">,</span>
        <span class="n">final_state</span> <span class="o">=</span> <span class="n">final_state</span><span class="p">,</span>
        <span class="n">total_loss</span> <span class="o">=</span> <span class="n">total_loss</span><span class="p">,</span>
        <span class="n">train_step</span> <span class="o">=</span> <span class="n">train_step</span>
    <span class="p">)</span></code></pre></figure>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">t</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">build_multilayer_lstm_graph_with_list</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="s">"It took"</span><span class="p">,</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t</span><span class="p">,</span> <span class="s">"seconds to build the graph."</span><span class="p">)</span></code></pre></figure>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">It</span> <span class="n">took</span> <span class="mf">25.640846967697144</span> <span class="n">seconds</span> <span class="n">to</span> <span class="n">build</span> <span class="n">the</span> <span class="n">graph</span><span class="p">.</span></code></pre></figure>

<p>Yikes, almost 30 seconds.</p>

<p>Now this isn’t that big of an issue for training, because we only need to build the graph once. It could be a big issue, however, if we need to build the graph multiple times at test time.</p>

<p>To get around this long compile time, Tensorflow allows us to create the graph at runtime. Here is a quick demonstration of the difference, using Tensorflow’s dynamic_rnn function:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">build_multilayer_lstm_graph_with_dynamic_rnn</span><span class="p">(</span>
    <span class="n">state_size</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
    <span class="n">num_classes</span> <span class="o">=</span> <span class="n">vocab_size</span><span class="p">,</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
    <span class="n">num_steps</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span>
    <span class="n">num_layers</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
    <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-4</span><span class="p">):</span>

    <span class="n">reset_graph</span><span class="p">()</span>

    <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">int32</span><span class="p">,</span> <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s">'input_placeholder'</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">int32</span><span class="p">,</span> <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s">'labels_placeholder'</span><span class="p">)</span>

    <span class="n">embeddings</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">'embedding_matrix'</span><span class="p">,</span> <span class="p">[</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">state_size</span><span class="p">])</span>

    <span class="c1"># Note that our inputs are no longer a list, but a tensor of dims batch_size x num_steps x state_size
</span>    <span class="n">rnn_inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">embedding_lookup</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

    <span class="n">cell</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">rnn_cell</span><span class="p">.</span><span class="n">LSTMCell</span><span class="p">(</span><span class="n">state_size</span><span class="p">,</span> <span class="n">state_is_tuple</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">cell</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">rnn_cell</span><span class="p">.</span><span class="n">MultiRNNCell</span><span class="p">([</span><span class="n">cell</span><span class="p">]</span> <span class="o">*</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">state_is_tuple</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">init_state</span> <span class="o">=</span> <span class="n">cell</span><span class="p">.</span><span class="n">zero_state</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">rnn_outputs</span><span class="p">,</span> <span class="n">final_state</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">dynamic_rnn</span><span class="p">(</span><span class="n">cell</span><span class="p">,</span> <span class="n">rnn_inputs</span><span class="p">,</span> <span class="n">initial_state</span><span class="o">=</span><span class="n">init_state</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s">'softmax'</span><span class="p">):</span>
        <span class="n">W</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">'W'</span><span class="p">,</span> <span class="p">[</span><span class="n">state_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">])</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">'b'</span><span class="p">,</span> <span class="p">[</span><span class="n">num_classes</span><span class="p">],</span> <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">constant_initializer</span><span class="p">(</span><span class="mf">0.0</span><span class="p">))</span>

    <span class="c1">#reshape rnn_outputs and y so we can get the logits in a single matmul
</span>    <span class="n">rnn_outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">rnn_outputs</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">state_size</span><span class="p">])</span>
    <span class="n">y_reshaped</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

    <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">rnn_outputs</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>

    <span class="n">total_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">sparse_softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y_reshaped</span><span class="p">))</span>
    <span class="n">train_step</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">).</span><span class="n">minimize</span><span class="p">(</span><span class="n">total_loss</span><span class="p">)</span>

    <span class="k">return</span> <span class="nb">dict</span><span class="p">(</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">,</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">,</span>
        <span class="n">init_state</span> <span class="o">=</span> <span class="n">init_state</span><span class="p">,</span>
        <span class="n">final_state</span> <span class="o">=</span> <span class="n">final_state</span><span class="p">,</span>
        <span class="n">total_loss</span> <span class="o">=</span> <span class="n">total_loss</span><span class="p">,</span>
        <span class="n">train_step</span> <span class="o">=</span> <span class="n">train_step</span>
    <span class="p">)</span></code></pre></figure>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">t</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">build_multilayer_lstm_graph_with_dynamic_rnn</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="s">"It took"</span><span class="p">,</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t</span><span class="p">,</span> <span class="s">"seconds to build the graph."</span><span class="p">)</span></code></pre></figure>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">It</span> <span class="n">took</span> <span class="mf">0.5314393043518066</span> <span class="n">seconds</span> <span class="n">to</span> <span class="n">build</span> <span class="n">the</span> <span class="n">graph</span><span class="p">.</span></code></pre></figure>

<p>Much better. One would think that pushing the graph construction to execution time would cause execution of the graph to go slower, but in this case, using dynamic_rnn actually speeds things up:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">g</span> <span class="o">=</span> <span class="n">build_multilayer_lstm_graph_with_list</span><span class="p">()</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">train_network</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"It took"</span><span class="p">,</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t</span><span class="p">,</span> <span class="s">"seconds to train for 3 epochs."</span><span class="p">)</span></code></pre></figure>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">Average</span> <span class="n">training</span> <span class="n">loss</span> <span class="k">for</span> <span class="n">Epoch</span> <span class="mi">0</span> <span class="p">:</span> <span class="mf">3.53323210245</span>
<span class="n">Average</span> <span class="n">training</span> <span class="n">loss</span> <span class="k">for</span> <span class="n">Epoch</span> <span class="mi">1</span> <span class="p">:</span> <span class="mf">3.31435756163</span>
<span class="n">Average</span> <span class="n">training</span> <span class="n">loss</span> <span class="k">for</span> <span class="n">Epoch</span> <span class="mi">2</span> <span class="p">:</span> <span class="mf">3.21755325109</span>
<span class="n">It</span> <span class="n">took</span> <span class="mf">117.78161263465881</span> <span class="n">seconds</span> <span class="n">to</span> <span class="n">train</span> <span class="k">for</span> <span class="mi">3</span> <span class="n">epochs</span><span class="p">.</span></code></pre></figure>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">g</span> <span class="o">=</span> <span class="n">build_multilayer_lstm_graph_with_dynamic_rnn</span><span class="p">()</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">train_network</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"It took"</span><span class="p">,</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t</span><span class="p">,</span> <span class="s">"seconds to train for 3 epochs."</span><span class="p">)</span></code></pre></figure>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">Average</span> <span class="n">training</span> <span class="n">loss</span> <span class="k">for</span> <span class="n">Epoch</span> <span class="mi">0</span> <span class="p">:</span> <span class="mf">3.55792756053</span>
<span class="n">Average</span> <span class="n">training</span> <span class="n">loss</span> <span class="k">for</span> <span class="n">Epoch</span> <span class="mi">1</span> <span class="p">:</span> <span class="mf">3.3225021006</span>
<span class="n">Average</span> <span class="n">training</span> <span class="n">loss</span> <span class="k">for</span> <span class="n">Epoch</span> <span class="mi">2</span> <span class="p">:</span> <span class="mf">3.28286816745</span>
<span class="n">It</span> <span class="n">took</span> <span class="mf">96.69413661956787</span> <span class="n">seconds</span> <span class="n">to</span> <span class="n">train</span> <span class="k">for</span> <span class="mi">3</span> <span class="n">epochs</span><span class="p">.</span></code></pre></figure>

<p>It’s not a breeze to work through and understand the dynamic_rnn code (which lives <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py">here</a>), but we can obtain a similar result ourselves by using tf.scan (dynamic_rnn does not use scan). Scan runs just a tad slower than Tensorflow’s optimized code, but is easier to understand and write yourself.</p>

<p>Scan is a higher-order function that you might be familiar with if you’ve done any programming in OCaml, Haskell or the like. In general, it takes a function $ (f: (x_t, y_{t-1}) \mapsto y_t) $, a sequence $ ([x_0, x_1 \dots x_n]) $ and an initial value $ (y_{-1}) $ and returns a sequence $ ([y_0, y_1 \dots y_n]) $ according to the rule: $ y_t = f(x_t, y_{t-1}) $. In Tensorflow, scan treats the first dimension of a Tensor as the sequence. Thus, if fed a Tensor of shape [n, m, o] as the sequence, scan would unpack it into a sequence of n-tensors, each with shape [m, o]. You can learn more about Tensorflow’s scan <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functional_ops.md#tfscanfn-elems-initializernone-parallel_iterations10-back_proptrue-swap_memoryfalse-namenone-scan">here</a>.</p>

<p>Below, I use scan with an LSTM so as to compare to the dynamic_rnn using Tensorflow above. Because LSTMs store their state in a 2-tuple, and we’re using a 3-layer network, the scan function produces, as <code class="language-plaintext highlighter-rouge">final_states below</code>, a 3-tuple (one for each layer) of 2-tuples (one for each LSTM state), each of shape [num_steps, batch_size, state_size]. We need only the last state, which is why we unpack, slice and repack <code class="language-plaintext highlighter-rouge">final_states</code> to get <code class="language-plaintext highlighter-rouge">final_state</code> below.</p>

<p>Another thing to note is that scan produces rnn_outputs with shape [num_steps, batch_size, state_size], whereas the dynamic_rnn produces rnn_outputs with shape [batch_size, num_steps, state_size] (the first two dimensions are switched). Dynamic_rnn has the flexibility to switch this behavior, using the “time_major” argument. Tf.scan does not have this flexibility, which is why we transpose <code class="language-plaintext highlighter-rouge">rnn_inputs</code> and <code class="language-plaintext highlighter-rouge">y</code> in the code below.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">build_multilayer_lstm_graph_with_scan</span><span class="p">(</span>
    <span class="n">state_size</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
    <span class="n">num_classes</span> <span class="o">=</span> <span class="n">vocab_size</span><span class="p">,</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
    <span class="n">num_steps</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span>
    <span class="n">num_layers</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
    <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-4</span><span class="p">):</span>

    <span class="n">reset_graph</span><span class="p">()</span>

    <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">int32</span><span class="p">,</span> <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s">'input_placeholder'</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">int32</span><span class="p">,</span> <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s">'labels_placeholder'</span><span class="p">)</span>

    <span class="n">embeddings</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">'embedding_matrix'</span><span class="p">,</span> <span class="p">[</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">state_size</span><span class="p">])</span>

    <span class="n">rnn_inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">embedding_lookup</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

    <span class="n">cell</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">rnn_cell</span><span class="p">.</span><span class="n">LSTMCell</span><span class="p">(</span><span class="n">state_size</span><span class="p">,</span> <span class="n">state_is_tuple</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">cell</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">rnn_cell</span><span class="p">.</span><span class="n">MultiRNNCell</span><span class="p">([</span><span class="n">cell</span><span class="p">]</span> <span class="o">*</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">state_is_tuple</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">init_state</span> <span class="o">=</span> <span class="n">cell</span><span class="p">.</span><span class="n">zero_state</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">rnn_outputs</span><span class="p">,</span> <span class="n">final_states</span> <span class="o">=</span> \
        <span class="n">tf</span><span class="p">.</span><span class="n">scan</span><span class="p">(</span><span class="k">lambda</span> <span class="n">a</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">cell</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                <span class="n">tf</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">rnn_inputs</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">]),</span>
                <span class="n">initializer</span><span class="o">=</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">state_size</span><span class="p">]),</span> <span class="n">init_state</span><span class="p">))</span>

    <span class="c1"># there may be a better way to do this:
</span>    <span class="n">final_state</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">([</span><span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">rnn_cell</span><span class="p">.</span><span class="n">LSTMStateTuple</span><span class="p">(</span>
                  <span class="n">tf</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nb">slice</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="p">[</span><span class="n">num_steps</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">state_size</span><span class="p">])),</span>
                  <span class="n">tf</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nb">slice</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="p">[</span><span class="n">num_steps</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">state_size</span><span class="p">])))</span>
                       <span class="k">for</span> <span class="n">c</span><span class="p">,</span> <span class="n">h</span> <span class="ow">in</span> <span class="n">final_states</span><span class="p">])</span>

    <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s">'softmax'</span><span class="p">):</span>
        <span class="n">W</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">'W'</span><span class="p">,</span> <span class="p">[</span><span class="n">state_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">])</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">'b'</span><span class="p">,</span> <span class="p">[</span><span class="n">num_classes</span><span class="p">],</span> <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">constant_initializer</span><span class="p">(</span><span class="mf">0.0</span><span class="p">))</span>

    <span class="n">rnn_outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">rnn_outputs</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">state_size</span><span class="p">])</span>
    <span class="n">y_reshaped</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">y</span><span class="p">,[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]),</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

    <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">rnn_outputs</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>

    <span class="n">total_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">sparse_softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y_reshaped</span><span class="p">))</span>
    <span class="n">train_step</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">).</span><span class="n">minimize</span><span class="p">(</span><span class="n">total_loss</span><span class="p">)</span>

    <span class="k">return</span> <span class="nb">dict</span><span class="p">(</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">,</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">,</span>
        <span class="n">init_state</span> <span class="o">=</span> <span class="n">init_state</span><span class="p">,</span>
        <span class="n">final_state</span> <span class="o">=</span> <span class="n">final_state</span><span class="p">,</span>
        <span class="n">total_loss</span> <span class="o">=</span> <span class="n">total_loss</span><span class="p">,</span>
        <span class="n">train_step</span> <span class="o">=</span> <span class="n">train_step</span>
    <span class="p">)</span></code></pre></figure>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">t</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">g</span> <span class="o">=</span> <span class="n">build_multilayer_lstm_graph_with_scan</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="s">"It took"</span><span class="p">,</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t</span><span class="p">,</span> <span class="s">"seconds to build the graph."</span><span class="p">)</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">train_network</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"It took"</span><span class="p">,</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t</span><span class="p">,</span> <span class="s">"seconds to train for 3 epochs."</span><span class="p">)</span></code></pre></figure>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">It</span> <span class="n">took</span> <span class="mf">0.6475389003753662</span> <span class="n">seconds</span> <span class="n">to</span> <span class="n">build</span> <span class="n">the</span> <span class="n">graph</span><span class="p">.</span>
<span class="n">Average</span> <span class="n">training</span> <span class="n">loss</span> <span class="k">for</span> <span class="n">Epoch</span> <span class="mi">0</span> <span class="p">:</span> <span class="mf">3.55362293501</span>
<span class="n">Average</span> <span class="n">training</span> <span class="n">loss</span> <span class="k">for</span> <span class="n">Epoch</span> <span class="mi">1</span> <span class="p">:</span> <span class="mf">3.32045680079</span>
<span class="n">Average</span> <span class="n">training</span> <span class="n">loss</span> <span class="k">for</span> <span class="n">Epoch</span> <span class="mi">2</span> <span class="p">:</span> <span class="mf">3.27433713688</span>
<span class="n">It</span> <span class="n">took</span> <span class="mf">101.60246014595032</span> <span class="n">seconds</span> <span class="n">to</span> <span class="n">train</span> <span class="k">for</span> <span class="mi">3</span> <span class="n">epochs</span><span class="p">.</span></code></pre></figure>

<p>Using scan was only marginally slower than using dynamic_rnn, and gives us the flexibility and understanding to tweak the code if we ever need to (e.g., if for some reason we wanted to create a skip connection from the state at timestep t-2 to timestep t, it would be easy to do with scan).</p>

  </div>

<!--<div id="disqus_thread"></div>
  <script>
    var disqus_config = function () {
      this.page.url = 'https://aadikuchlous.github.io/jekyll/tensorflow/neuralnetworks/2022/01/04/recurrent-neural-networks-in-tensorflow-ii.html';
      this.page.identifier = 'https://aadikuchlous.github.io/jekyll/tensorflow/neuralnetworks/2022/01/04/recurrent-neural-networks-in-tensorflow-ii.html';
    };

    (function() {
      var d = document, s = d.createElement('script');

      s.src = 'https://maneeshblogtest.disqus.com/embed.js';

      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>--><script defer src="https://cdn.commento.io/js/commento.js"></script>
  <noscript>Please enable JavaScript to load the comments.</noscript>
  <div id="commento"></div>
<a class="u-url" href="/jekyll/tensorflow/neuralnetworks/2022/01/04/recurrent-neural-networks-in-tensorflow-ii.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Your awesome title</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Your awesome title</li>
<li><a class="u-email" href="mailto:your-email@example.com">your-email@example.com</a></li>
</ul>
      </div>

      <div class="footer-col footer-col-2">
<ul class="social-media-list">
<li><a href="https://github.com/jekyll"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">jekyll</span></a></li>
<li><a href="https://www.twitter.com/jekyllrb"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">jekyllrb</span></a></li>
</ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
